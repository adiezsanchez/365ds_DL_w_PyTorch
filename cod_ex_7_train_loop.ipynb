{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af910c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 100/100 [00:00<00:00, 3426.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 48.6344\n",
      "Epoch [2/100], Loss: 33.7873\n",
      "Epoch [3/100], Loss: 23.4849\n",
      "Epoch [4/100], Loss: 16.3361\n",
      "Epoch [5/100], Loss: 11.3754\n",
      "Epoch [6/100], Loss: 7.9331\n",
      "Epoch [7/100], Loss: 5.5443\n",
      "Epoch [8/100], Loss: 3.8865\n",
      "Epoch [9/100], Loss: 2.7360\n",
      "Epoch [10/100], Loss: 1.9374\n",
      "Epoch [11/100], Loss: 1.3831\n",
      "Epoch [12/100], Loss: 0.9982\n",
      "Epoch [13/100], Loss: 0.7309\n",
      "Epoch [14/100], Loss: 0.5452\n",
      "Epoch [15/100], Loss: 0.4161\n",
      "Epoch [16/100], Loss: 0.3263\n",
      "Epoch [17/100], Loss: 0.2638\n",
      "Epoch [18/100], Loss: 0.2202\n",
      "Epoch [19/100], Loss: 0.1897\n",
      "Epoch [20/100], Loss: 0.1683\n",
      "Epoch [21/100], Loss: 0.1533\n",
      "Epoch [22/100], Loss: 0.1426\n",
      "Epoch [23/100], Loss: 0.1350\n",
      "Epoch [24/100], Loss: 0.1295\n",
      "Epoch [25/100], Loss: 0.1255\n",
      "Epoch [26/100], Loss: 0.1225\n",
      "Epoch [27/100], Loss: 0.1202\n",
      "Epoch [28/100], Loss: 0.1184\n",
      "Epoch [29/100], Loss: 0.1169\n",
      "Epoch [30/100], Loss: 0.1157\n",
      "Epoch [31/100], Loss: 0.1146\n",
      "Epoch [32/100], Loss: 0.1137\n",
      "Epoch [33/100], Loss: 0.1128\n",
      "Epoch [34/100], Loss: 0.1120\n",
      "Epoch [35/100], Loss: 0.1113\n",
      "Epoch [36/100], Loss: 0.1106\n",
      "Epoch [37/100], Loss: 0.1099\n",
      "Epoch [38/100], Loss: 0.1092\n",
      "Epoch [39/100], Loss: 0.1085\n",
      "Epoch [40/100], Loss: 0.1078\n",
      "Epoch [41/100], Loss: 0.1072\n",
      "Epoch [42/100], Loss: 0.1065\n",
      "Epoch [43/100], Loss: 0.1059\n",
      "Epoch [44/100], Loss: 0.1053\n",
      "Epoch [45/100], Loss: 0.1046\n",
      "Epoch [46/100], Loss: 0.1040\n",
      "Epoch [47/100], Loss: 0.1034\n",
      "Epoch [48/100], Loss: 0.1028\n",
      "Epoch [49/100], Loss: 0.1021\n",
      "Epoch [50/100], Loss: 0.1015\n",
      "Epoch [51/100], Loss: 0.1009\n",
      "Epoch [52/100], Loss: 0.1003\n",
      "Epoch [53/100], Loss: 0.0997\n",
      "Epoch [54/100], Loss: 0.0991\n",
      "Epoch [55/100], Loss: 0.0985\n",
      "Epoch [56/100], Loss: 0.0979\n",
      "Epoch [57/100], Loss: 0.0974\n",
      "Epoch [58/100], Loss: 0.0968\n",
      "Epoch [59/100], Loss: 0.0962\n",
      "Epoch [60/100], Loss: 0.0956\n",
      "Epoch [61/100], Loss: 0.0951\n",
      "Epoch [62/100], Loss: 0.0945\n",
      "Epoch [63/100], Loss: 0.0939\n",
      "Epoch [64/100], Loss: 0.0934\n",
      "Epoch [65/100], Loss: 0.0928\n",
      "Epoch [66/100], Loss: 0.0922\n",
      "Epoch [67/100], Loss: 0.0917\n",
      "Epoch [68/100], Loss: 0.0911\n",
      "Epoch [69/100], Loss: 0.0906\n",
      "Epoch [70/100], Loss: 0.0901\n",
      "Epoch [71/100], Loss: 0.0895\n",
      "Epoch [72/100], Loss: 0.0890\n",
      "Epoch [73/100], Loss: 0.0885\n",
      "Epoch [74/100], Loss: 0.0879\n",
      "Epoch [75/100], Loss: 0.0874\n",
      "Epoch [76/100], Loss: 0.0869\n",
      "Epoch [77/100], Loss: 0.0864\n",
      "Epoch [78/100], Loss: 0.0858\n",
      "Epoch [79/100], Loss: 0.0853\n",
      "Epoch [80/100], Loss: 0.0848\n",
      "Epoch [81/100], Loss: 0.0843\n",
      "Epoch [82/100], Loss: 0.0838\n",
      "Epoch [83/100], Loss: 0.0833\n",
      "Epoch [84/100], Loss: 0.0828\n",
      "Epoch [85/100], Loss: 0.0823\n",
      "Epoch [86/100], Loss: 0.0818\n",
      "Epoch [87/100], Loss: 0.0813\n",
      "Epoch [88/100], Loss: 0.0808\n",
      "Epoch [89/100], Loss: 0.0804\n",
      "Epoch [90/100], Loss: 0.0799\n",
      "Epoch [91/100], Loss: 0.0794\n",
      "Epoch [92/100], Loss: 0.0789\n",
      "Epoch [93/100], Loss: 0.0785\n",
      "Epoch [94/100], Loss: 0.0780\n",
      "Epoch [95/100], Loss: 0.0775\n",
      "Epoch [96/100], Loss: 0.0771\n",
      "Epoch [97/100], Loss: 0.0766\n",
      "Epoch [98/100], Loss: 0.0761\n",
      "Epoch [99/100], Loss: 0.0757\n",
      "Epoch [100/100], Loss: 0.0752\n",
      "\n",
      "Learned parameters:\n",
      "linear.weight: tensor([[1.7724]])\n",
      "linear.bias: tensor([0.6692])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Define a simple linear regression model.\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# 2. Create some dummy training data (features X and labels y).\n",
    "X_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]], dtype=torch.float32)\n",
    "y_train = torch.tensor([[2.0], [4.0], [6.0], [8.0]], dtype=torch.float32)\n",
    "\n",
    "# 3. Instantiate the LinearRegression model with input and output dimensions of 1.\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "model = LinearRegression(input_dim, output_dim)\n",
    "\n",
    "# 4. Define the loss function (Mean Squared Error Loss).\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# 5. Define the optimizer (Stochastic Gradient Descent) with a learning rate of 0.01.\n",
    "learning_rate = 0.01\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 6. Implement the training loop. Run for a few epochs (e.g., 10).\n",
    "num_epochs = 100\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    # Forward pass: compute predictions on the training data.\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # Calculate the loss between the predictions and the true labels.\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    # Backward pass: compute the gradients of the loss with respect to the model parameters.\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()       # Compute gradients\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    # Print the loss for each epoch to observe the training progress.\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 7. After training, print the learned parameters (weights and bias) of the linear layer.\n",
    "print(\"\\nLearned parameters:\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "microglia_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
